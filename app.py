from flask import Flask, render_template, request, jsonify
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import tensorflow as tf
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
import pickle
from urllib.parse import urlparse

# –°–∫–∞—á–∏–≤–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã NLTK
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

app = Flask(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
model = None
tokenizer = None
MAX_SEQUENCE_LENGTH = 300

def load_ml_components():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
    global model, tokenizer
    try:
        model = load_model('cnn_lstm_model.h5')
        with open('tokenizer.pkl', 'rb') as handle:
            tokenizer = pickle.load(handle)
        print("‚úÖ –ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        return True
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

def analyze_source(url):
    """–ê–Ω–∞–ª–∏–∑ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –¥–æ–º–µ–Ω—É"""
    if not url or url.strip() == '':
        return 0.5, "–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ —É–∫–∞–∑–∞–Ω"
    
    try:
        domain = urlparse(url).netloc.lower()
        
        # –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–∞–¥–µ–∂–Ω—ã—Ö –∏ –Ω–µ–Ω–∞–¥–µ–∂–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
        TRUSTED_DOMAINS = {
            'bbc.com', 'reuters.com', 'apnews.com', 'cnn.com', 
            'nytimes.com', 'theguardian.com', 'washingtonpost.com',
            'npr.org', 'wsj.com', 'bloomberg.com', 'bbc.co.uk' , 'edition.cnn.com'
        }
        
        FAKE_NEWS_DOMAINS = {
            'horrorzone.ru', 'fakenews.com', 'clickbait.com',
            'conspiracy.com', 'satire.com', 'theonion.com'
        }
        
        SUSPICIOUS_DOMAINS = {
            'blogspot.com', 'wordpress.com', 'weebly.com', 'tumblr.com'
        }
        
        if domain in TRUSTED_DOMAINS:
            return 0.9, f"–ù–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {domain}"
        elif domain in FAKE_NEWS_DOMAINS:
            return 0.1, f"–ù–µ–Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {domain}"
        elif domain in SUSPICIOUS_DOMAINS:
            return 0.3, f"–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {domain}"
        else:
            return 0.5, f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {domain}"
            
    except Exception as e:
        return 0.5, f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {str(e)}"

def preprocess_text(text):
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏–¥–µ–Ω—Ç–∏—á–Ω–∞—è –æ–±—É—á–µ–Ω–∏—é"""
    if text is None or text == '':
        return ''
    
    text_str = str(text).strip()
    if text_str == '' or text_str == 'nan':
        return ''
    
    # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    text_clean = re.sub(r'[^a-zA-Z\s]', '', text_str, re.I|re.A)
    text_clean = text_clean.lower().strip()
    
    if not text_clean:
        return ''
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = text_clean.split()
    
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤ –∏ –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è
    lemmatizer = WordNetLemmatizer()
    stop_words = set(stopwords.words('english'))
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    
    return ' '.join(tokens)

def predict_news(text, source_trust=0.5):
    """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –≤–≤–µ–¥–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å —É—á–µ—Ç–æ–º –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    if model is None or tokenizer is None:
        raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
    cleaned_text = preprocess_text(text)
    
    if not cleaned_text.strip():
        return "RELIABLE", 0.5, 0.5
    
    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –ø–∞–¥–¥–∏–Ω–≥
    sequence = tokenizer.texts_to_sequences([cleaned_text])
    padded_sequence = pad_sequences(sequence, maxlen=MAX_SEQUENCE_LENGTH, 
                                  padding='post', truncating='post')
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
    raw_prediction = model.predict(padded_sequence, verbose=0)[0][0]
    print(f"üîç –°—ã—Ä–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {raw_prediction:.4f}")
    print(f"üåê –î–æ–≤–µ—Ä–∏–µ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É: {source_trust:.2f}")
    
    # –ê–ù–ê–õ–ò–ó –¢–ï–ö–°–¢–ê
    text_lower = cleaned_text.lower()
    
    # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ñ–µ–π–∫–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    FAKE_INDICATORS = [
        'shock', 'dragons', 'miracle', 'cure', 'trick', 'eliminates', 
        'doctors hate', 'big pharma', 'secret', 'shocking', 'instant',
        'wonder', 'magic', 'breakthrough', 'hidden', 'lose 10kg',
        '3 days', 'doctors are shocked', 'click to discover', 'simple trick',
        'cancer', 'disease', 'weight loss', 'burn fat', 'overnight'
    ]
    
    # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–µ–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
    REAL_INDICATORS = [
        'according to', 'report', 'confirmed', 'official', 'research',
        'study', 'experts', 'meteorological', 'weather service',
        'temperature', 'degrees', 'celsius', 'forecast', 'scientific',
        'observation', 'data', 'analysis', 'ministry', 'government'
    ]
    
    fake_score = sum(1 for word in FAKE_INDICATORS if word in text_lower)
    real_score = sum(1 for word in REAL_INDICATORS if word in text_lower)
    
    print(f"üìä –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞: —Ñ–µ–π–∫-–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤={fake_score}, —Ä–µ–∞–ª—å–Ω—ã—Ö={real_score}")
    
    # –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ù–û–ï –†–ï–®–ï–ù–ò–ï (—Ç–µ–∫—Å—Ç + –∏—Å—Ç–æ—á–Ω–∏–∫)
    text_analysis_score = fake_score - real_score
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ = 60% –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ + 40% –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–∞
    combined_score = (text_analysis_score * 0.6) + ((1 - source_trust) * 0.4)
    
    print(f"üìà –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {combined_score:.2f}")
    
    # –ü–†–ò–ù–Ø–¢–ò–ï –†–ï–®–ï–ù–ò–Ø
    if combined_score > 1.0 or fake_score >= 3:
        result = "UNRELIABLE"
        confidence = min(0.95, 0.7 + (combined_score * 0.1))
        print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢: –§–ï–ô–ö (—Å–∏–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)")
        
    elif combined_score < -1.0 or real_score >= 3:
        result = "RELIABLE"
        confidence = min(0.95, 0.7 + (abs(combined_score) * 0.1))
        print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢: –†–ï–ê–õ–¨–ù–ê–Ø (—Å–∏–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)")
        
    else:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å —Å –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π
        correction = 0.3
        model_score = max(0.05, raw_prediction - correction)
        
        if model_score > 0.5:
            result = "UNRELIABLE"
            confidence = model_score
        else:
            result = "RELIABLE"
            confidence = 1 - model_score
        
        print(f"üéØ –†–ï–ó–£–õ–¨–¢–ê–¢: –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–æ–¥–µ–ª–∏ —Å –∫–æ—Ä—Ä–µ–∫—Ü–∏–µ–π")
    
    print(f"üìä –§–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {result}")
    print(f"üíØ –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.2%}")
    
    # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    final_score = confidence if result == "UNRELIABLE" else 1 - confidence
    
    return result, float(confidence), float(final_score)

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/predict', methods=['POST'])
def predict():
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –ª–∏ –º–æ–¥–µ–ª—å
        if model is None or tokenizer is None:
            return render_template('index.html', 
                                 prediction_text="–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.",
                                 prediction_class="error")
        
        news_text = request.form.get('news_text', '').strip()
        news_url = request.form.get('news_url', '').strip()  # –ü–æ–ª—É—á–∞–µ–º URL –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—è
        
        if not news_text:
            return render_template('index.html', 
                                 prediction_text="–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏",
                                 prediction_class="warning")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        source_trust, source_info = analyze_source(news_url)
        print(f"üåê {source_info}")
        
        # –ü–µ—Ä–µ–¥–∞–µ–º —Ç–µ–∫—Å—Ç –∏ –¥–æ–≤–µ—Ä–∏–µ –∫ –∏—Å—Ç–æ—á–Ω–∏–∫—É
        result, confidence, raw_prob = predict_news(text=news_text, source_trust=source_trust)
        confidence_percent = confidence * 100
        
        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_text = "–§–ï–ô–ö" if result == "UNRELIABLE" else "–†–ï–ê–õ–¨–ù–ê–Ø"
        emoji = "‚ùå" if result == "UNRELIABLE" else "‚úÖ"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å—Ç–æ—á–Ω–∏–∫–µ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        source_display = f" ({source_info})" if news_url else ""
        
        return render_template('index.html',
                             prediction_text=f'{emoji} –ù–æ–≤–æ—Å—Ç—å: {result_text}{source_display}',
                             prediction_class='unreliable' if result == 'UNRELIABLE' else 'reliable',
                             confidence_text=f'–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence_percent:.2f}%',
                             confidence_percent=confidence_percent,
                             news_text=news_text,
                             news_url=news_url)
    
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤ predict: {e}")
        return render_template('index.html',
                             prediction_text=f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(e)}',
                             prediction_class='error')

@app.route('/api/predict', methods=['POST'])
def api_predict():
    """API endpoint –¥–ª—è JSON –∑–∞–ø—Ä–æ—Å–æ–≤"""
    try:
        if model is None or tokenizer is None:
            return jsonify({'error': '–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞', 'status': 'error'}), 503
        
        data = request.get_json()
        if not data or 'text' not in data:
            return jsonify({'error': '–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –ø–æ–ª–µ "text"', 'status': 'error'}), 400
        
        news_text = data.get('text', '').strip()
        news_url = data.get('url', '').strip()
        
        if not news_text:
            return jsonify({'error': '–¢–µ–∫—Å—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º', 'status': 'error'}), 400
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫
        source_trust, source_info = analyze_source(news_url)
        
        result, confidence, raw_prob = predict_news(text=news_text, source_trust=source_trust)
        
        return jsonify({
            'result': result,
            'confidence': confidence,
            'probability': raw_prob,
            'source_trust': source_trust,
            'source_info': source_info,
            'status': 'success',
            'is_fake': result == 'UNRELIABLE'
        })
    
    except Exception as e:
        return jsonify({'error': str(e), 'status': 'error'}), 400

@app.route('/health')
def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    status = {
        'status': 'healthy' if model is not None and tokenizer is not None else 'unhealthy',
        'model_loaded': model is not None,
        'tokenizer_loaded': tokenizer is not None,
        'max_sequence_length': MAX_SEQUENCE_LENGTH
    }
    return jsonify(status)

if __name__ == '__main__':
    print("üöÄ –ó–∞–ø—É—Å–∫ Flask –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
    success = load_ml_components()
    
    if not success:
        print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤:")
        print("   - cnn_lstm_model.h5")
        print("   - tokenizer.pkl")
    
    app.run(debug=True, host='0.0.0.0', port=5000)